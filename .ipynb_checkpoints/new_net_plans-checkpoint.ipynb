{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#imports\" data-toc-modified-id=\"imports-0.1\"><span class=\"toc-item-num\">0.1&nbsp;&nbsp;</span>imports</a></span></li><li><span><a href=\"#methods\" data-toc-modified-id=\"methods-0.2\"><span class=\"toc-item-num\">0.2&nbsp;&nbsp;</span>methods</a></span></li></ul></li><li><span><a href=\"#Planning-new-network\" data-toc-modified-id=\"Planning-new-network-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Planning new network</a></span></li><li><span><a href=\"#go-through-the-steps-to-make-sure-i-remember\" data-toc-modified-id=\"go-through-the-steps-to-make-sure-i-remember-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>go through the steps to make sure i remember</a></span><ul class=\"toc-item\"><li><span><a href=\"#re-organize-data\" data-toc-modified-id=\"re-organize-data-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>re-organize data</a></span></li><li><span><a href=\"#get_pairs()\" data-toc-modified-id=\"get_pairs()-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>get_pairs()</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from collections import defaultdict\n",
    "from glob import glob\n",
    "from random import choice, sample\n",
    "\n",
    "# Florencia's imports\n",
    "import tensorflow\n",
    "from tensorflow.keras import layers, preprocessing, models, optimizers\n",
    "from tensorflow.keras.layers import Input, Conv2D, Lambda, Dense, Flatten,MaxPooling2D,Activation, Dropout, BatchNormalization,  GlobalMaxPool2D, GlobalAvgPool2D, Concatenate, Subtract,Multiply\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from skimage.io import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "from random import sample, choice\n",
    "\n",
    "from tensorflow.keras import backend as K \n",
    "from keras.preprocessing.image import ImageDataGenerator \n",
    "from keras.backend import clear_session\n",
    "from random import randint\n",
    "from random import uniform as rfloat\n",
    "import h5py\n",
    "from tensorflow.keras.models import model_from_json \n",
    "from keras import regularizers\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = np.array(img).astype(np.float)\n",
    "    return preprocess_input(img)\n",
    "def paths_to_arrays(paths_list):\n",
    "    out = []\n",
    "    for path in paths_list:\n",
    "        out.append(read_img(path))\n",
    "    return out\n",
    "\n",
    "def get_jpg_lists(): # hardcoded ;(\n",
    "    dir_path = os.getcwd()\n",
    "    train_part = '/UTKface/train/'\n",
    "    val_part = '/UTKface/validation/'\n",
    "    test_part = '/UTKface/test/'\n",
    "    # no overlap between train, val, test imgs\n",
    "    train_path = dir_path+train_part # 2008\n",
    "    val_path = dir_path+val_part # 95\n",
    "    test_path = dir_path+test_part # 155, distinct from val\n",
    "    \n",
    "    trains = glob(train_path + '*.jpg')\n",
    "    tests = glob(test_path + '*.jpg')\n",
    "    vals = glob(val_path + '*.jpg')\n",
    "    return trains, tests, vals\n",
    "\n",
    "def jpgs_to_arrays(trains, tests, vals):\n",
    "    train_data = paths_to_arrays(trains)\n",
    "    test_data = paths_to_arrays(tests)\n",
    "    val_data = paths_to_arrays(vals)\n",
    "    return train_data, test_data, val_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Planning new network\n",
    "- 2 limiting factors from last model:\n",
    "    - data\n",
    "        - 2,000 images is good, but i don't think augmentation will lead to great learning\n",
    "        - use something like AT&Tfaces, which has ~40 'users' but 8 photos of each user\n",
    "            - this is few-shot learning after all, the simulated extra shots may not be giving me the desired output\n",
    "            - then again, it could also be:\n",
    "    - architecture size\n",
    "        - 6 layers, mixture of dense+conv+flattening. messed around with different activations\n",
    "        - i think i should try changing this first, because it's something i want to learn for future models as well\n",
    "- Model updates:\n",
    "    - transfer learning\n",
    "        - try importing VGGnet, Facenet etc and running\n",
    "        - learn Sagemaker to run it so i'm not limited by local computing power\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# go through the steps to make sure i remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## re-organize data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_pairs()\n",
    "core function is get_pairs().\n",
    "the net intakes 2 images and outputs 1 or 0 for same or different label.\n",
    "so our x is a list of [img_array_a, img_array_b] pairs, and our y is a list of [1, 0, 1] matching each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(imgs_list): # creates 50/50 split of 'same' and 'dif' image pairs & labels. len=n*5\n",
    "    pairs = [] # x\n",
    "    labels = [] # y\n",
    "    print(f'making pairs from {len(imgs_list)} images...')\n",
    "    for c, img in enumerate(imgs_list):\n",
    "        temp = imgs_list # shallow copy messed this up\n",
    "        for i in range(0,4): # make 4 'same' and 'dif' pairs for each base_image\n",
    "            # same pair\n",
    "            pairs.append([img, img])\n",
    "            labels.append(1) # true\n",
    "            # dif pair\n",
    "            print(f'getting rand_ind between 0 and {len(temp)}')\n",
    "            indx = randint(0,len(temp)-1)\n",
    "            while indx==c: # if random index matches current iteration index\n",
    "                indx = randint(0,len(temp)-1) # avoid deepcopy memory time\n",
    "            print(f'index: {indx}')\n",
    "            pairs.append([img, temp[indx]]) # make a \"dif\" pair of images\n",
    "            labels.append(0)\n",
    "            print(f'total {len(pairs)}')\n",
    "        print(f'done with image #{c} \\n')\n",
    "    return pairs, labels # returns lists for x, y. pairs+labels matched up by index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- process\n",
    "    - get all images in one big list\n",
    "        - train_test_split into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xy_split(train_list, test_list, val_list):\n",
    "    x_train, y_train = get_pairs(train_data)\n",
    "    x_test, y_test = get_pairs(test_data)\n",
    "    x_val, y_val = get_pairs(val_data)\n",
    "    return x_train,y_train, x_test,y_test, x_val,y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2008, 200, 200, 3)\n",
      "(155,)\n",
      "(95, 200, 200, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data, test_data, val_data = get_jpg_lists()\n",
    "train_data, test_data, val_data = jpgs_to_arrays(train_data, test_data, val_data)\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(test_data)) # shape glitch. maybe a corrupted jpg?\n",
    "print(np.shape(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the test jpg issue\n",
    "def avoid_ghosts(train_jpgs, n=160): # 2008-160=1848. both fit batch_size=16\n",
    "    tests = random.sample(train_jpgs, n)\n",
    "    for jpg in tests:\n",
    "        if jpg in train_jpgs:\n",
    "            train_jpgs.remove(jpg)\n",
    "    return tests\n",
    "test_data = avoid_ghosts(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
